{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f999af93",
   "metadata": {},
   "source": [
    "## Exercise 3: Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f95645",
   "metadata": {},
   "source": [
    "*In this task, you will perform text classification on the 20 newsgroups dataset. It is a collection of e-mails coming from\n",
    "different newsgroups. The goal is to assign each e-mail to its corresponding newsgroup.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952eb955",
   "metadata": {},
   "source": [
    "*We build a basic CNN for this task using the keras library*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3caa864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c76e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "#             2.1 Creating Data Splits\n",
    "# ------------------------------------------------\n",
    "\n",
    "################################\n",
    "input_file = 'data.txt'\n",
    "################################\n",
    "\n",
    "tmp_dir = '/tmp'\n",
    "train_verbose = 1\n",
    "pad_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8350f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_file):\n",
    "    vocab = {0}\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    with open(input_file) as f:\n",
    "        for line in f:\n",
    "            label, content = line.split('\\t')\n",
    "            content = [int(v) for v in content.split()]\n",
    "            vocab.update(content)\n",
    "            data_x.append(content)\n",
    "            label = tuple(int(v) for v in label.split())\n",
    "            data_y.append(label)\n",
    "\n",
    "    data_x = pad_sequences(data_x, maxlen=pad_length)\n",
    "    return list(zip(data_y, data_x)), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d01c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, vocab = read_data(input_file)\n",
    "vocab_size = max(vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "190eda9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seeds\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "random.shuffle(data)\n",
    "input_len = len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2e23769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y: a list of 20-component one-hot vectors representing newsgroups\n",
    "# train_x: a list of 300-component vectors where each entry corresponds to a word ID\n",
    "train_y, train_x = zip(*(data[:(input_len * 8) // 10]))\n",
    "dev_y, dev_x = zip(*(data[(input_len * 8) // 10: (input_len * 9) // 10]))\n",
    "test_y, test_x = zip(*(data[(input_len * 9) // 10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "decfb6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave those unmodified and, if requested by the task, modify them locally in the specific task\n",
    "batch_size = 64\n",
    "embedding_dims = 100\n",
    "epochs = 2\n",
    "filters = 75\n",
    "kernel_size = 3  # Keras uses a different definition where a kernel size of 3 means that 3 words are convolved at each step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c3fd1b",
   "metadata": {},
   "source": [
    "*Keras CNN model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3daadcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_50 (Embedding)    (None, 300, 300)          25855200  \n",
      "                                                                 \n",
      " reshape_11 (Reshape)        (None, 300, 300, 1)       0         \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 299, 299, 75)      375       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 149, 149, 75)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1665075)           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                33301520  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59,157,095\n",
      "Trainable params: 59,157,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,300  ,input_length=pad_length))\n",
    "model.add(Reshape((300,300,1),input_shape=trainx.shape[1:]))\n",
    "model.add(Conv2D(75,kernel_size=(2,2),activation='relu',input_shape= (None,300,100,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5fa06176",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y)\n",
    "dev_x = np.asarray(dev_x)\n",
    "dev_y = np.asarray(dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, verbose=train_verbose, validation_data=(dev_x, dev_y))\n",
    "print('Accuracy of simple CNN: %f\\n' % model.evaluate(dev_x, dev_y, verbose=0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6aa0a",
   "metadata": {},
   "source": [
    "*After training the basic CNN on Google collab we got Accuracy on the dev set: 0.783786*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969002b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "earlystopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")\n",
    "          \n",
    "model.fit(train_x, train_y, batch_size=batch_size, epochs=50, verbose=train_verbose, validation_data=(dev_x,dev_y),callbacks=[earlystopping_callback,model_checkpoint_callback])\n",
    "model.load_weights('weights')\n",
    "print('Accuracy of best model on dev set: %f\\n' % model.evaluate(dev_x, dev_y, verbose=0)[1])\n",
    "print('Accuracy of best model on test set: %f\\n' % model.evaluate(test_x, test_y, verbose=0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893fc1c9",
   "metadata": {},
   "source": [
    "*After applying early stopping and model checkpoint we get:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c9a226",
   "metadata": {},
   "source": [
    "*dev test accuracy of best model: 0.817313 Test set accuracy of best model: 0.809878*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
